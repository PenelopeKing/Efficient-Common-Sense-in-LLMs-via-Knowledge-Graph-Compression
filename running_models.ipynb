{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/anlg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from basic_trainer import get_basic_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=f\"{DATA_PATH}/train.source\"\n",
    "target_path=f\"{DATA_PATH}/train.target\"\n",
    "test_source_path=f\"{DATA_PATH}/test.source\"\n",
    "test_target_path=f\"{DATA_PATH}/test.target\"\n",
    "model_name = \"facebook/bart-base\"\n",
    "output_dir = \"Final_Model_No_KG_anlg\"\n",
    "max_len = 128\n",
    "epochs = 6\n",
    "train_batch_size = 20\n",
    "num_points = 25_597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3560/3560 [00:00<00:00, 17467.93 examples/s]\n",
      "Map:   0%|          | 0/25597 [00:00<?, ? examples/s]c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25597/25597 [00:10<00:00, 2337.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3560/3560 [00:02<00:00, 1384.50 examples/s]\n",
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\quent\\OneDrive\\Desktop\\Code\\Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression\\basic_trainer.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = get_basic_trainer(\n",
    "    source_path=source_path,\n",
    "    target_path=target_path,\n",
    "    test_source_path = test_source_path,\n",
    "    test_target_path = test_target_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    max_len=max_len,\n",
    "    epochs=epochs,\n",
    "    train_batch_size=train_batch_size,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7680' max='7680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7680/7680 22:39, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Avg Loss Min</th>\n",
       "      <th>Avg Loss Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.045700</td>\n",
       "      <td>1.845009</td>\n",
       "      <td>1.204555</td>\n",
       "      <td>1.792630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.768600</td>\n",
       "      <td>1.828380</td>\n",
       "      <td>1.162525</td>\n",
       "      <td>1.773439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.553600</td>\n",
       "      <td>1.813573</td>\n",
       "      <td>1.146367</td>\n",
       "      <td>1.766641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.422200</td>\n",
       "      <td>1.833336</td>\n",
       "      <td>1.148597</td>\n",
       "      <td>1.788359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.289500</td>\n",
       "      <td>1.874858</td>\n",
       "      <td>1.147620</td>\n",
       "      <td>1.821632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.225800</td>\n",
       "      <td>1.888603</td>\n",
       "      <td>1.150975</td>\n",
       "      <td>1.835164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7680, training_loss=1.5686609675486882, metrics={'train_runtime': 1360.0743, 'train_samples_per_second': 112.917, 'train_steps_per_second': 5.647, 'total_flos': 1623394738667520.0, 'train_loss': 1.5686609675486882, 'epoch': 6.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#79 minutes for 20 epochs = 4 minutes per epoch\n",
    "#22 minute 40 seconds for 6 epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Final_Model_No_KG\\\\tokenizer_config.json',\n",
       " 'Final_Model_No_KG\\\\special_tokens_map.json',\n",
       " 'Final_Model_No_KG\\\\vocab.json',\n",
       " 'Final_Model_No_KG\\\\merges.txt',\n",
       " 'Final_Model_No_KG\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG no compression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cpnet...\n",
      "Done\n",
      "Loaded 3947 allowed concept IDs from train/val data.\n"
     ]
    }
   ],
   "source": [
    "from KG_trainer_no_comp import get_KG_trainer, generate_explanation_no_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=f\"{DATA_PATH}/train.source\"\n",
    "target_path=f\"{DATA_PATH}/train.target\"\n",
    "test_source_path=f\"{DATA_PATH}/test.source\"\n",
    "test_target_path=f\"{DATA_PATH}/test.target\"\n",
    "model_name = \"facebook/bart-base\"\n",
    "output_dir = \"Final_Model_No_Comp\"\n",
    "max_len = 128\n",
    "epochs = 6\n",
    "train_batch_size = 20\n",
    "num_points = 25_597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data:   0%|          | 0/25596 [00:00<?, ? examples/s]c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Preprocessing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25596/25596 [18:10<00:00, 23.48 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed dataset to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25596/25596 [00:00<00:00, 995423.36 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input: <s>He poured orange juice on his cereal. <KG> poured water milk cement underground concrete air earth ground land bread burn flow oil rock sand sky soap steam food ice refine wave bacteria bay boat current diver dolphin fish island lizard lobster ocean oxygen salt submarine tuna turtle backpack bath bathroom beer bottle bridge bubble bucket closet coffee container cooler dentist dip drain drink drip flood fog fountain glass grape gym iceberg jar jug juice lake liter mouth mud pee perfume pipe pool rain rainbow restaurant river roof sewer shower sink snow snowball soda soup surface cold liquid beverage fluid coat dry paste rice root seal skim stone drinking form stuff white material orange red apple lemon peel opaque sweet color colour fruit</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\OneDrive\\Desktop\\Code\\Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression\\KG_trainer_no_comp.py:121: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = get_KG_trainer(\n",
    "    source_path=source_path,\n",
    "    target_path=target_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    max_len=max_len,\n",
    "    epochs=epochs,\n",
    "    train_batch_size=train_batch_size,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24 minutes and 46 seconds\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGCN comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cpnet...\n",
      "Done\n",
      "Loaded 3947 allowed concept IDs from train/val data.\n"
     ]
    }
   ],
   "source": [
    "from KG_trainer_w_comp2 import get_KG_RGCN_trainer, get_graph_info, BartGraphAwareForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=f\"{DATA_PATH}/train.source\"\n",
    "target_path=f\"{DATA_PATH}/train.target\"\n",
    "model_name = \"facebook/bart-base\"\n",
    "output_dir = \"FINAL_Model_RGCN\"\n",
    "max_len = 128\n",
    "epochs = 6\n",
    "train_batch_size = 20\n",
    "num_points = 25_596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartGraphAwareForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['graph_encoder.embedding.weight', 'graph_encoder.gcn.rgcn.bias', 'graph_encoder.gcn.rgcn.root', 'graph_encoder.gcn.rgcn.weight', 'graph_encoder.sag_pool.attn.bias', 'graph_encoder.sag_pool.attn.weight', 'graph_encoder.sag_pool.scoring.gcn.rgcn.bias', 'graph_encoder.sag_pool.scoring.gcn.rgcn.root', 'graph_encoder.sag_pool.scoring.gcn.rgcn.weight', 'graph_encoder.sag_pool.scoring.linear.bias', 'graph_encoder.sag_pool.scoring.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\quent\\OneDrive\\Desktop\\Code\\Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression\\KG_trainer_w_comp2.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = get_KG_RGCN_trainer(\n",
    "    source_path=source_path,\n",
    "    target_path=target_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    max_len=max_len,\n",
    "    epochs=epochs,\n",
    "    train_batch_size=train_batch_size,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='7680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  56/7680 00:50 < 1:59:24, 1.06 it/s, Epoch 0.04/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.018300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\trainer.py:2550\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m-> 2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cpnet...\n",
      "Done\n",
      "Loaded 3947 allowed concept IDs from train/val data.\n"
     ]
    }
   ],
   "source": [
    "from KG_trainer_w_transformer2 import get_KG_transformer_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=f\"{DATA_PATH}/train.source\"\n",
    "target_path=f\"{DATA_PATH}/train.target\"\n",
    "model_name = \"facebook/bart-base\"\n",
    "output_dir = \"FINAL_Model_Transformer_Anlg\"\n",
    "max_len = 128\n",
    "epochs = 6\n",
    "train_batch_size = 12\n",
    "num_points = 25_596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Some weights of BartGraphAwareForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['graph_encoder.embedding.weight', 'graph_encoder.gcn.rgcn.bias', 'graph_encoder.gcn.rgcn.root', 'graph_encoder.gcn.rgcn.weight', 'graph_encoder.global_transform.layers.0.linear1.bias', 'graph_encoder.global_transform.layers.0.linear1.weight', 'graph_encoder.global_transform.layers.0.linear2.bias', 'graph_encoder.global_transform.layers.0.linear2.weight', 'graph_encoder.global_transform.layers.0.norm1.bias', 'graph_encoder.global_transform.layers.0.norm1.weight', 'graph_encoder.global_transform.layers.0.norm2.bias', 'graph_encoder.global_transform.layers.0.norm2.weight', 'graph_encoder.global_transform.layers.0.self_attn.in_proj_bias', 'graph_encoder.global_transform.layers.0.self_attn.in_proj_weight', 'graph_encoder.global_transform.layers.0.self_attn.out_proj.bias', 'graph_encoder.global_transform.layers.0.self_attn.out_proj.weight', 'graph_encoder.sag_pool.attn.bias', 'graph_encoder.sag_pool.attn.weight', 'graph_encoder.sag_pool.scoring.gcn.rgcn.bias', 'graph_encoder.sag_pool.scoring.gcn.rgcn.root', 'graph_encoder.sag_pool.scoring.gcn.rgcn.weight', 'graph_encoder.sag_pool.scoring.linear.bias', 'graph_encoder.sag_pool.scoring.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\quent\\OneDrive\\Desktop\\Code\\Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression\\KG_trainer_w_transformer2.py:430: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = get_KG_transformer_trainer(\n",
    "    source_path=source_path,\n",
    "    target_path=target_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    max_len=max_len,\n",
    "    epochs=epochs,\n",
    "    train_batch_size=train_batch_size,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12798' max='12798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12798/12798 1:47:20, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.347700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.948100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.987300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.738700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.797600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.545500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.418800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.277800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12798, training_loss=1.720411757600179, metrics={'train_runtime': 6441.8263, 'train_samples_per_second': 23.84, 'train_steps_per_second': 1.987, 'total_flos': 4231185775873920.0, 'train_loss': 1.720411757600179, 'epoch': 6.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#65 minutes 18 seconds\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cpnet...\n",
      "Done\n",
      "Loaded 1 allowed concept IDs from train/val data.\n"
     ]
    }
   ],
   "source": [
    "from KG_trainer_w_transformer3 import get_KG_transformer_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path=f\"{DATA_PATH}/train.source\"\n",
    "target_path=f\"{DATA_PATH}/train.target\"\n",
    "model_name = \"facebook/bart-base\"\n",
    "output_dir = \"FINAL_Model_Transformer_large\"\n",
    "max_len = 128\n",
    "epochs = 6\n",
    "train_batch_size = 20\n",
    "num_points = 25_596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\anaconda3\\envs\\capstone_scratch_2\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Some weights of BartGraphAwareForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['graph_encoder.embedding.weight', 'graph_encoder.rgcn1.rgcn.bias', 'graph_encoder.rgcn1.rgcn.root', 'graph_encoder.rgcn1.rgcn.weight', 'graph_encoder.rgcn2.rgcn.bias', 'graph_encoder.rgcn2.rgcn.root', 'graph_encoder.rgcn2.rgcn.weight', 'graph_encoder.sag_pool.attn.bias', 'graph_encoder.sag_pool.attn.weight', 'graph_encoder.sag_pool.scoring.gcn.rgcn.bias', 'graph_encoder.sag_pool.scoring.gcn.rgcn.root', 'graph_encoder.sag_pool.scoring.gcn.rgcn.weight', 'graph_encoder.sag_pool.scoring.linear.bias', 'graph_encoder.sag_pool.scoring.linear.weight', 'graph_encoder.transformer1.layers.0.linear1.bias', 'graph_encoder.transformer1.layers.0.linear1.weight', 'graph_encoder.transformer1.layers.0.linear2.bias', 'graph_encoder.transformer1.layers.0.linear2.weight', 'graph_encoder.transformer1.layers.0.norm1.bias', 'graph_encoder.transformer1.layers.0.norm1.weight', 'graph_encoder.transformer1.layers.0.norm2.bias', 'graph_encoder.transformer1.layers.0.norm2.weight', 'graph_encoder.transformer1.layers.0.self_attn.in_proj_bias', 'graph_encoder.transformer1.layers.0.self_attn.in_proj_weight', 'graph_encoder.transformer1.layers.0.self_attn.out_proj.bias', 'graph_encoder.transformer1.layers.0.self_attn.out_proj.weight', 'graph_encoder.transformer2.layers.0.linear1.bias', 'graph_encoder.transformer2.layers.0.linear1.weight', 'graph_encoder.transformer2.layers.0.linear2.bias', 'graph_encoder.transformer2.layers.0.linear2.weight', 'graph_encoder.transformer2.layers.0.norm1.bias', 'graph_encoder.transformer2.layers.0.norm1.weight', 'graph_encoder.transformer2.layers.0.norm2.bias', 'graph_encoder.transformer2.layers.0.norm2.weight', 'graph_encoder.transformer2.layers.0.self_attn.in_proj_bias', 'graph_encoder.transformer2.layers.0.self_attn.in_proj_weight', 'graph_encoder.transformer2.layers.0.self_attn.out_proj.bias', 'graph_encoder.transformer2.layers.0.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\quent\\OneDrive\\Desktop\\Code\\Efficient-Common-Sense-in-LLMs-via-Knowledge-Graph-Compression\\KG_trainer_w_transformer3.py:434: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = get_KG_transformer_trainer(\n",
    "    source_path=source_path,\n",
    "    target_path=target_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    max_len=max_len,\n",
    "    epochs=epochs,\n",
    "    train_batch_size=train_batch_size,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from bleu import Bleu\n",
    "from corpus_diversity import eval_entropy_distinct\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = Bleu()\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"Final_Model_No_KG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the trained model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation(model, tokenizer, sentence: str) -> list:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    # Generate output using beam search:\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=60,\n",
    "        num_beams=3,\n",
    "        num_return_sequences=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Decode each of the generated token sequences\n",
    "    explanations = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in output_ids\n",
    "    ]\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(input, explanation, ground_truths):\n",
    "    diversity_scores = eval_entropy_distinct(explanation)\n",
    "    distinct_2 = diversity_scores[\"distinct_2\"]*100\n",
    "    entropy_4 = diversity_scores[\"entropy_4\"]\n",
    "\n",
    "    self_BLEU_3 = 0\n",
    "    self_BLEU_4 = 0\n",
    "    for i, e in enumerate(explanation):\n",
    "        res = {input: [e]}\n",
    "        gts = {input: explanation[:i] + explanation[i+1:]}\n",
    "        scores = bleu.compute_score(gts = gts, res = res)[0]\n",
    "        self_BLEU_3 += scores[2]\n",
    "        self_BLEU_4 += scores[3]\n",
    "    self_BLEU_3 = (self_BLEU_3/len(explanation))*100\n",
    "    self_BLEU_4 = (self_BLEU_4/len(explanation))*100\n",
    "\n",
    "    bleu_4 = []\n",
    "    for e in explanation:\n",
    "        res = {input: [e]}\n",
    "        bleu_4 += [bleu.compute_score(gts = ground_truths, res = res)[0][3]]\n",
    "    bleu_4 = max(bleu_4)*100\n",
    "\n",
    "    rouge_l = []\n",
    "    for e in explanation:\n",
    "        res = {input: [e]}\n",
    "        rouge_l += [rouge.compute_score(gts = ground_truths, res = res)[0]]\n",
    "    rouge_l = max(rouge_l)*100\n",
    "\n",
    "    return {\"self_bleu_3\": self_BLEU_3, \"self_blue_4\": self_BLEU_4, \"distinct_2\": distinct_2, \\\n",
    "            \"entropy_4\": entropy_4, \"bleu_4\": bleu_4, \"rouge_l\": rouge_l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with no KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [01:31<00:00, 10.86sample/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_PATH}/test.source\", \"r\", encoding=\"utf-8\") as f_src, open(f\"{DATA_PATH}/test.target\", \"r\", encoding=\"utf-8\") as f_tgt:\n",
    "    test_inputs = [line.strip() for line in f_src]\n",
    "    test_targets = [line.strip().split(\"\\t\") for line in f_tgt]\n",
    "\n",
    "total_metrics = {\n",
    "    \"self_bleu_3\": 0,\n",
    "    \"self_blue_4\": 0,\n",
    "    \"distinct_2\": 0,\n",
    "    \"entropy_4\": 0,\n",
    "    \"bleu_4\": 0,\n",
    "    \"rouge_l\": 0\n",
    "}\n",
    "num_samples = len(test_inputs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text, ground_truths in tqdm(zip(test_inputs, test_targets), total=num_samples, desc=\"Evaluating\", unit=\"sample\"):\n",
    "    explanations = generate_explanation(model, tokenizer, input_text)\n",
    "    metrics = get_metrics(input_text, explanations, {input_text: ground_truths})\n",
    "    \n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] += metrics[key]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "average_metrics = {key: total_metrics[key] / num_samples for key in total_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self_bleu_3': 49.355684637815955,\n",
       " 'self_blue_4': 37.80137361269743,\n",
       " 'distinct_2': 67.43213703263366,\n",
       " 'entropy_4': 2.0438268725548494,\n",
       " 'bleu_4': 14.663010594547837,\n",
       " 'rouge_l': 49.278712200164534}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.38669443130493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with No Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"Final_Model_No_Comp/checkpoint-7680\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the trained model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [01:59<00:00,  8.31sample/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_PATH}/test.source\", \"r\", encoding=\"utf-8\") as f_src, \\\n",
    "     open(f\"{DATA_PATH}/test.target\", \"r\", encoding=\"utf-8\") as f_tgt:\n",
    "    test_inputs = [line.strip() for line in f_src]\n",
    "    test_targets = [line.strip().split(\"\\t\") for line in f_tgt]\n",
    "\n",
    "total_metrics = {\n",
    "    \"self_bleu_3\": 0,\n",
    "    \"self_blue_4\": 0,\n",
    "    \"distinct_2\": 0,\n",
    "    \"entropy_4\": 0,\n",
    "    \"bleu_4\": 0,\n",
    "    \"rouge_l\": 0\n",
    "}\n",
    "num_samples = len(test_inputs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text, ground_truths in tqdm(zip(test_inputs, test_targets),\n",
    "                                        total=num_samples, desc=\"Evaluating\", unit=\"sample\"):\n",
    "    # Use the model's built-in generate_answer method instead of generate_explanation.\n",
    "    answers = generate_explanation_no_comp(model, tokenizer, device, input_text)\n",
    "    metrics = get_metrics(input_text, answers, {input_text: ground_truths})\n",
    "    \n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] += metrics[key]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "average_metrics = {key: total_metrics[key] / num_samples for key in total_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self_bleu_3': 49.88682780945167,\n",
       " 'self_blue_4': 37.78526990568948,\n",
       " 'distinct_2': 66.99712690941175,\n",
       " 'entropy_4': 2.057265039656348,\n",
       " 'bleu_4': 15.295446878590887,\n",
       " 'rouge_l': 49.83796165440578}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.41376757621765"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGCN Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation_RGCN(model, tokenizer, sentence: str, max_length=60, device=\"cuda\"):\n",
    "    # Tokenize the input text and move to device\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Ensure inputs are on device\n",
    "    \n",
    "    # Extract graph info from the sentence and move graph tensors to device\n",
    "    graph_info = get_graph_info(sentence)\n",
    "    concept_ids = graph_info[\"concept_ids\"].to(device).unsqueeze(0) if graph_info[\"concept_ids\"].numel() > 0 else None\n",
    "    edge_index = graph_info[\"edge_index\"].to(device).unsqueeze(0) if graph_info[\"edge_index\"].numel() > 0 else None\n",
    "    edge_type = graph_info[\"edge_type\"].to(device).unsqueeze(0) if graph_info[\"edge_type\"].numel() > 0 else None\n",
    "\n",
    "    # Generate outputs using beam search; passing the graph fields as additional arguments\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        concept_ids=concept_ids,\n",
    "        edge_index=edge_index,\n",
    "        edge_type=edge_type,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,\n",
    "        num_return_sequences=3,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode the generated token sequences\n",
    "    explanations = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "    return explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"Final_Model_RGCN/checkpoint-7680\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "model = BartGraphAwareForConditionalGeneration.from_pretrained(model_dir, tokenizer=tokenizer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grocery stores are too small to be parachuted into.',\n",
       " 'Grocery stores are too big to be parachuted into.',\n",
       " \"Grocery stores don't have parachute.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_explanation_RGCN(model, tokenizer, \"She parachuted into the grocery store from a plane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [02:12<00:00,  7.51sample/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_PATH}/test.source\", \"r\", encoding=\"utf-8\") as f_src, \\\n",
    "     open(f\"{DATA_PATH}/test.target\", \"r\", encoding=\"utf-8\") as f_tgt:\n",
    "    test_inputs = [line.strip() for line in f_src]\n",
    "    test_targets = [line.strip().split(\"\\t\") for line in f_tgt]\n",
    "\n",
    "total_metrics = {\n",
    "    \"self_bleu_3\": 0,\n",
    "    \"self_blue_4\": 0,\n",
    "    \"distinct_2\": 0,\n",
    "    \"entropy_4\": 0,\n",
    "    \"bleu_4\": 0,\n",
    "    \"rouge_l\": 0\n",
    "}\n",
    "num_samples = len(test_inputs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text, ground_truths in tqdm(zip(test_inputs, test_targets),\n",
    "                                        total=num_samples, desc=\"Evaluating\", unit=\"sample\"):\n",
    "    # Use the model's built-in generate_answer method instead of generate_explanation.\n",
    "    answers = generate_explanation_RGCN(model, tokenizer, input_text)\n",
    "    metrics = get_metrics(input_text, answers, {input_text: ground_truths})\n",
    "    \n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] += metrics[key]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "average_metrics = {key: total_metrics[key] / num_samples for key in total_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self_bleu_3': 51.70947840574771,\n",
       " 'self_blue_4': 40.07679656740238,\n",
       " 'distinct_2': 66.13550050967883,\n",
       " 'entropy_4': 2.1173555207600954,\n",
       " 'bleu_4': 15.208048969180071,\n",
       " 'rouge_l': 49.804082801475026}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.04450726509094"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KG_trainer_w_transformer2 import get_KG_transformer_trainer, get_graph_info, BartGraphAwareForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"FINAL_Model_Transformer_2/checkpoint-15000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "model = BartGraphAwareForConditionalGeneration.from_pretrained(model_dir, tokenizer=tokenizer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Grocery stores don't have landing strips for planes.\",\n",
       " 'Grocery stores do not have landing strips for planes.',\n",
       " \"Grocery stores don't have landing strips.\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_explanation_RGCN(model, tokenizer, \"She parachuted into the grocery store from a plane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [03:59<00:00,  4.14sample/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_PATH}/test.source\", \"r\", encoding=\"utf-8\") as f_src, \\\n",
    "     open(f\"{DATA_PATH}/test.target\", \"r\", encoding=\"utf-8\") as f_tgt:\n",
    "    test_inputs = [line.strip() for line in f_src]\n",
    "    test_targets = [line.strip().split(\"\\t\") for line in f_tgt]\n",
    "\n",
    "total_metrics = {\n",
    "    \"self_bleu_3\": 0,\n",
    "    \"self_blue_4\": 0,\n",
    "    \"distinct_2\": 0,\n",
    "    \"entropy_4\": 0,\n",
    "    \"bleu_4\": 0,\n",
    "    \"rouge_l\": 0\n",
    "}\n",
    "num_samples = len(test_inputs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text, ground_truths in tqdm(zip(test_inputs, test_targets),\n",
    "                                        total=num_samples, desc=\"Evaluating\", unit=\"sample\"):\n",
    "    # Use the model's built-in generate_answer method instead of generate_explanation.\n",
    "    answers = generate_explanation_RGCN(model, tokenizer, input_text)\n",
    "    metrics = get_metrics(input_text, answers, {input_text: ground_truths})\n",
    "    \n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] += metrics[key]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "average_metrics = {key: total_metrics[key] / num_samples for key in total_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'self_bleu_3': 46.69938289730443,\n",
       " 'self_blue_4': 36.499709275801415,\n",
       " 'distinct_2': 69.62484104278442,\n",
       " 'entropy_4': 2.165679227604565,\n",
       " 'bleu_4': 12.145884723922116,\n",
       " 'rouge_l': 47.15472672038363}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239.408935546875"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KG_trainer_w_transformer3 import get_KG_transformer_trainer, get_graph_info, BartGraphAwareForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"Final_Model_Transformer_large/checkpoint-7680\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_dir)\n",
    "model = BartGraphAwareForConditionalGeneration.from_pretrained(model_dir, tokenizer=tokenizer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Grocery stores don't have parachutes.\",\n",
       " 'Grocery stores are too small to fit in a plane.',\n",
       " 'Grocery stores are too small to be parachuted into.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_explanation_RGCN(model, tokenizer, \"She parachuted into the grocery store from a plane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [02:48<00:00,  5.89sample/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_PATH}/test.source\", \"r\", encoding=\"utf-8\") as f_src, \\\n",
    "     open(f\"{DATA_PATH}/test.target\", \"r\", encoding=\"utf-8\") as f_tgt:\n",
    "    test_inputs = [line.strip() for line in f_src]\n",
    "    test_targets = [line.strip().split(\"\\t\") for line in f_tgt]\n",
    "\n",
    "total_metrics = {\n",
    "    \"self_bleu_3\": 0,\n",
    "    \"self_blue_4\": 0,\n",
    "    \"distinct_2\": 0,\n",
    "    \"entropy_4\": 0,\n",
    "    \"bleu_4\": 0,\n",
    "    \"rouge_l\": 0\n",
    "}\n",
    "num_samples = len(test_inputs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text, ground_truths in tqdm(zip(test_inputs, test_targets),\n",
    "                                        total=num_samples, desc=\"Evaluating\", unit=\"sample\"):\n",
    "    # Use the model's built-in generate_answer method instead of generate_explanation.\n",
    "    answers = generate_explanation_RGCN(model, tokenizer, input_text)\n",
    "    metrics = get_metrics(input_text, answers, {input_text: ground_truths})\n",
    "    \n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] += metrics[key]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "average_metrics = {key: total_metrics[key] / num_samples for key in total_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maverage_metrics\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'average_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.38669443130493"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_scratch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
