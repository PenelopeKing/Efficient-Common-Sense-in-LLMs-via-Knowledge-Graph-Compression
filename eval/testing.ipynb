{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import copy\n",
    "import sys, math, re\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "from six.moves import xrange as range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bleu and rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc180/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I  can count  stars.\n",
      "Generated Explanation: ['Stars are not real.', 'Stars cannot be counted.', 'stars are not real.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "def generate_explanation(model, tokenizer, sentence: str) -> list:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    # Generate output using beam search:\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=60,\n",
    "        num_beams=3,\n",
    "        num_return_sequences=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Decode each of the generated token sequences\n",
    "    explanations = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in output_ids\n",
    "    ]\n",
    "    return explanations\n",
    "\n",
    "FINE_TUNED_MODEL_DIR = \"basic_finetuned_out/checkpoint-1281\"\n",
    "tokenizer = BartTokenizer.from_pretrained(FINE_TUNED_MODEL_DIR)\n",
    "model = BartForConditionalGeneration.from_pretrained(FINE_TUNED_MODEL_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "test_sentence = \"I  can count  stars.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: We use book to know the time.\n",
      "Generated Explanation: ['Book is used to read, not to tell time.', 'Book is used to read, not to know time.', 'Book is used to read and not to tell time.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We use book to know the time.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "ground_truths = ['A book is used to study.'\t,'A book does not have the ability to show what time it is.',\t\"Books don't tell the time.\"]\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precook(s, n=4, out=False):\n",
    "    \"\"\"Takes a string as input and returns an object that can be given to\n",
    "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
    "    can take string arguments as well.\"\"\"\n",
    "    words = s.split()\n",
    "    counts = defaultdict(int)\n",
    "    for k in range(1,n+1):\n",
    "        for i in range(len(words)-k+1):\n",
    "            ngram = tuple(words[i:i+k])\n",
    "            counts[ngram] += 1\n",
    "    return (len(words), counts)\n",
    "\n",
    "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.'''\n",
    "\n",
    "    reflen = []\n",
    "    maxcounts = {}\n",
    "    for ref in refs:\n",
    "        rl, counts = precook(ref, n)\n",
    "        reflen.append(rl)\n",
    "        for (ngram,count) in six.iteritems(counts):\n",
    "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "    if eff == \"shortest\":\n",
    "        reflen = min(reflen)\n",
    "    elif eff == \"average\":\n",
    "        reflen = float(sum(reflen))/len(reflen)\n",
    "\n",
    "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
    "\n",
    "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
    "\n",
    "    return (reflen, maxcounts)\n",
    "\n",
    "def cook_test(test, reflen_refmaxcounts, eff=None, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.'''\n",
    "\n",
    "    reflen, refmaxcounts = reflen_refmaxcounts\n",
    "    testlen, counts = precook(test, n, True)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "\n",
    "    if eff == \"closest\":\n",
    "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
    "    else: ## i.e., \"average\" or \"shortest\" or None\n",
    "        result[\"reflen\"] = reflen\n",
    "\n",
    "    result[\"testlen\"] = testlen\n",
    "\n",
    "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
    "\n",
    "    result['correct'] = [0]*n\n",
    "    for (ngram, count) in six.iteritems(counts):\n",
    "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
    "\n",
    "    return result\n",
    "\n",
    "class BleuScorer(object):\n",
    "    \"\"\"Bleu scorer.\"\"\"\n",
    "\n",
    "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
    "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
    "\n",
    "    def copy(self):\n",
    "        ''' copy the refs.'''\n",
    "        new = BleuScorer(n=self.n)\n",
    "        new.ctest = copy.copy(self.ctest)\n",
    "        new.crefs = copy.copy(self.crefs)\n",
    "        new._score = None\n",
    "        return new\n",
    "\n",
    "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
    "        ''' singular instance '''\n",
    "\n",
    "        self.n = n\n",
    "        self.crefs = []\n",
    "        self.ctest = []\n",
    "        self.cook_append(test, refs)\n",
    "        self.special_reflen = special_reflen\n",
    "\n",
    "    def cook_append(self, test, refs):\n",
    "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
    "\n",
    "        if refs is not None:\n",
    "            self.crefs.append(cook_refs(refs))\n",
    "            if test is not None:\n",
    "                cooked_test = cook_test(test, self.crefs[-1])\n",
    "                self.ctest.append(cooked_test) ## N.B.: -1\n",
    "            else:\n",
    "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
    "\n",
    "        self._score = None ## need to recompute\n",
    "\n",
    "    def ratio(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._ratio\n",
    "\n",
    "    def score_ratio(self, option=None):\n",
    "        '''return (bleu, len_ratio) pair'''\n",
    "        return (self.fscore(option=option), self.ratio(option=option))\n",
    "\n",
    "    def score_ratio_str(self, option=None):\n",
    "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
    "\n",
    "    def reflen(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._reflen\n",
    "\n",
    "    def testlen(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._testlen\n",
    "\n",
    "    def retest(self, new_test):\n",
    "        if type(new_test) is str:\n",
    "            new_test = [new_test]\n",
    "        assert len(new_test) == len(self.crefs), new_test\n",
    "        self.ctest = []\n",
    "        for t, rs in zip(new_test, self.crefs):\n",
    "            self.ctest.append(cook_test(t, rs))\n",
    "        self._score = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rescore(self, new_test):\n",
    "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
    "\n",
    "        return self.retest(new_test).compute_score()\n",
    "\n",
    "    def size(self):\n",
    "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
    "        return len(self.crefs)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''add an instance (e.g., from another sentence).'''\n",
    "\n",
    "        if type(other) is tuple:\n",
    "            ## avoid creating new BleuScorer instances\n",
    "            self.cook_append(other[0], other[1])\n",
    "        else:\n",
    "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
    "            self.ctest.extend(other.ctest)\n",
    "            self.crefs.extend(other.crefs)\n",
    "            self._score = None ## need to recompute\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compatible(self, other):\n",
    "        return isinstance(other, BleuScorer) and self.n == other.n\n",
    "\n",
    "    def single_reflen(self, option=\"average\"):\n",
    "        return self._single_reflen(self.crefs[0][0], option)\n",
    "\n",
    "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
    "\n",
    "        if option == \"shortest\":\n",
    "            reflen = min(reflens)\n",
    "        elif option == \"average\":\n",
    "            reflen = float(sum(reflens))/len(reflens)\n",
    "        elif option == \"closest\":\n",
    "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
    "        else:\n",
    "            assert False, \"unsupported reflen option %s\" % option\n",
    "\n",
    "        return reflen\n",
    "\n",
    "    def recompute_score(self, option=None, verbose=0):\n",
    "        self._score = None\n",
    "        return self.compute_score(option, verbose)\n",
    "\n",
    "    def compute_score(self, option=None, verbose=0):\n",
    "        n = self.n\n",
    "        small = 1e-9\n",
    "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
    "        bleu_list = [[] for _ in range(n)]\n",
    "\n",
    "        if self._score is not None:\n",
    "            return self._score\n",
    "\n",
    "        if option is None:\n",
    "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
    "\n",
    "        self._testlen = 0\n",
    "        self._reflen = 0\n",
    "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
    "\n",
    "        # for each sentence\n",
    "        for comps in self.ctest:\n",
    "            testlen = comps['testlen']\n",
    "            self._testlen += testlen\n",
    "\n",
    "            if self.special_reflen is None: ## need computation\n",
    "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
    "            else:\n",
    "                reflen = self.special_reflen\n",
    "\n",
    "            self._reflen += reflen\n",
    "\n",
    "            for key in ['guess','correct']:\n",
    "                for k in range(n):\n",
    "                    totalcomps[key][k] += comps[key][k]\n",
    "\n",
    "            # append per image bleu score\n",
    "            bleu = 1.\n",
    "            for k in range(n):\n",
    "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
    "                        /(float(comps['guess'][k]) + small)\n",
    "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
    "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
    "            if ratio < 1:\n",
    "                for k in range(n):\n",
    "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(comps, reflen)\n",
    "\n",
    "        totalcomps['reflen'] = self._reflen\n",
    "        totalcomps['testlen'] = self._testlen\n",
    "\n",
    "        bleus = []\n",
    "        bleu = 1.\n",
    "        for k in range(n):\n",
    "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
    "                    / (totalcomps['guess'][k] + small)\n",
    "            bleus.append(bleu ** (1./(k+1)))\n",
    "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
    "        if ratio < 1:\n",
    "            for k in range(n):\n",
    "                bleus[k] *= math.exp(1 - 1/ratio)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(totalcomps)\n",
    "            print(\"ratio:\", ratio)\n",
    "\n",
    "        self._score = bleus\n",
    "        return self._score, bleu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        # default compute Blue score up to 4\n",
    "        self._n = n\n",
    "        self._hypo_for_image = {}\n",
    "        self.ref_for_image = {}\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        bleu_scorer = BleuScorer(n=self._n)\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) >= 1)\n",
    "\n",
    "            bleu_scorer += (hypo[0], ref)\n",
    "\n",
    "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
    "        # print(\"closest:\", score)\n",
    "        # print(\"shortest:\", bleu_scorer.compute_score(option='shortest'))\n",
    "        # print(\"average:\", bleu_scorer.compute_score(option='average', verbose=1))\n",
    "        # return (bleu, bleu_info)\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"Bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: We use book to know the time.\n",
      "Generated Explanation: ['Book is used to read, not to tell time.', 'Book is used to read, not to know time.', 'Book is used to read and not to tell time.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We use book to know the time.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "ground_truths = {test_sentence: ['A book is used to study.'\t,'A book does not have the ability to show what time it is.',\t\"Books don't tell the time.\"]}\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\n",
      "\n",
      "Book is used to read, not to tell time.\n",
      "[0.6666666665925927, 0.4082482904156672, 0.28768479129605307, 4.463236137244506e-05]\n",
      "\n",
      "Book is used to read, not to know time.\n",
      "[0.5555555554938273, 0.37267799620596836, 0.27072175357070527, 4.264366797236323e-05]\n",
      "\n",
      "Book is used to read and not to tell time.\n",
      "[0.44449093232013265, 0.27050856692987363, 0.1892319966923457, 2.910042506738825e-05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bleu = Bleu()\n",
    "print('[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\\n')\n",
    "for e in explanation:\n",
    "    print(e)\n",
    "    res = {test_sentence: [e]}\n",
    "    print(bleu.compute_score(gts = ground_truths, res = res)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def my_lcs(string, sub):\n",
    "    \"\"\"\n",
    "    Calculates longest common subsequence for a pair of tokenized strings\n",
    "    :param string : list of str : tokens from a string split using whitespace\n",
    "    :param sub : list of str : shorter string, also split using whitespace\n",
    "    :returns: length (list of int): length of the longest common subsequence between the two strings\n",
    "\n",
    "    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n",
    "    \"\"\"\n",
    "    if(len(string)< len(sub)):\n",
    "        sub, string = string, sub\n",
    "\n",
    "    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
    "\n",
    "    for j in range(1,len(sub)+1): \n",
    "        for i in range(1,len(string)+1):\n",
    "            if(string[i-1] == sub[j-1]):\n",
    "                lengths[i][j] = lengths[i-1][j-1] + 1\n",
    "            else:\n",
    "                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
    "\n",
    "    return lengths[len(string)][len(sub)]\n",
    "\n",
    "class Rouge():\n",
    "    '''\n",
    "    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # vrama91: updated the value below based on discussion with Hovey\n",
    "        self.beta = 1.2\n",
    "\n",
    "    def calc_score(self, candidate, refs):\n",
    "        \"\"\"\n",
    "        Compute ROUGE-L score given one candidate and references for an image\n",
    "        :param candidate: str : candidate sentence to be evaluated\n",
    "        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n",
    "        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n",
    "        \"\"\"\n",
    "        assert(len(candidate)==1)\t\n",
    "        assert(len(refs)>0)         \n",
    "        prec = []\n",
    "        rec = []\n",
    "\n",
    "        # split into tokens\n",
    "        token_c = candidate[0].split(\" \")\n",
    "    \t\n",
    "        for reference in refs:\n",
    "            # split into tokens\n",
    "            token_r = reference.split(\" \")\n",
    "            # compute the longest common subsequence\n",
    "            lcs = my_lcs(token_r, token_c)\n",
    "            prec.append(lcs/float(len(token_c)))\n",
    "            rec.append(lcs/float(len(token_r)))\n",
    "\n",
    "        prec_max = max(prec)\n",
    "        rec_max = max(rec)\n",
    "\n",
    "        if(prec_max!=0 and rec_max !=0):\n",
    "            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n",
    "        else:\n",
    "            score = 0.0\n",
    "        return score\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n",
    "        Invoked by evaluate_captions.py \n",
    "        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values \n",
    "        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n",
    "        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n",
    "        \"\"\"\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        score = []\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref  = gts[id]\n",
    "\n",
    "            score.append(self.calc_score(hypo, ref))\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) > 0)\n",
    "\n",
    "        average_score = np.mean(np.array(score))\n",
    "        return average_score, np.array(score)\n",
    "\n",
    "    def method(self):\n",
    "        return \"Rouge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns ROUGE-L Score... how good of match is it with the references?\n",
      "\n",
      "Book is used to read, not to tell time.\n",
      "(0.4149659863945578, array([0.41496599]))\n",
      "\n",
      "Book is used to read, not to know time.\n",
      "(0.4149659863945578, array([0.41496599]))\n",
      "\n",
      "Book is used to read and not to tell time.\n",
      "(0.3927038626609442, array([0.39270386]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "print('Returns ROUGE-L Score... how good of match is it with the references?\\n')\n",
    "\n",
    "for e in explanation:\n",
    "    print(e)\n",
    "    res = {test_sentence: [e]}\n",
    "    print(rouge.compute_score(gts = ground_truths, res = res))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: We use book to know the time.\n",
      "Generated Explanation: ['Book is used to read, not to tell time.', 'Book is used to read, not to know time.', 'Book is used to read and not to tell time.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We use book to know the time.\"\n",
    "explanations = generate_explanation(model, tokenizer, test_sentence)\n",
    "ground_truths = {test_sentence: ['A book is used to study.'\t,'A book does not have the ability to show what time it is.',\t\"Books don't tell the time.\"]}\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\n",
      "\n",
      "Book is used to read, not to tell time.\n",
      "{'We use book to know the time.': ['Book is used to read, not to know time.', 'Book is used to read and not to tell time.']}\n",
      "[0.9999999997777782, 0.9999999997708338, 0.9999999997625665, 0.9554427919678762]\n",
      "\n",
      "Book is used to read, not to know time.\n",
      "{'We use book to know the time.': ['Book is used to read, not to tell time.', 'Book is used to read and not to tell time.']}\n",
      "[0.8888888886913584, 0.8164965807406125, 0.780896665433669, 0.7506238535645454]\n",
      "\n",
      "Book is used to read and not to tell time.\n",
      "{'We use book to know the time.': ['Book is used to read, not to tell time.', 'Book is used to read, not to know time.']}\n",
      "[0.7999999999200001, 0.7302967432631348, 0.6436595896649729, 0.5253819788219215]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bleu = Bleu()\n",
    "\n",
    "print('[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\\n')\n",
    "for i, e in enumerate(explanations):\n",
    "    print(e)\n",
    "    res = {test_sentence: [e]}\n",
    "    gts = { test_sentence: explanations[:i] + explanations[i+1:]}\n",
    "    print(gts)\n",
    "    print(bleu.compute_score(gts = gts, res = res)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
