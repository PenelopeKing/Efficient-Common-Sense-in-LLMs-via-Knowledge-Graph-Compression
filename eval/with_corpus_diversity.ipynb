{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import copy\n",
    "import sys, math, re\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "from six.moves import xrange as range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bleu and rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I  can count  stars.\n",
      "Generated Explanation: ['There is no way to count stars.', 'There are no stars in the sky.', 'There are no stars.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from KG_trainer_w_comp import get_KG_trainer\n",
    "from KG_trainer_w_comp import BartGraphAwareForConditionalGeneration\n",
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "\n",
    "def generate_explanation(model, tokenizer, sentence: str) -> list:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    # Generate output using beam search:\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=60,\n",
    "        num_beams=3,\n",
    "        num_return_sequences=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Decode each of the generated token sequences\n",
    "    explanations = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in output_ids\n",
    "    ]\n",
    "    return explanations\n",
    "\n",
    "model_path = \"KG_finetuned_out2/checkpoint-30\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BartGraphAwareForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "test_sentence = \"I  can count  stars.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: We use book to know the time.\n",
      "Generated Explanation: ['Book is used to know the time.', 'Book is a book.', 'Book is used to know time.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We use book to know the time.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "ground_truths = ['A book is used to study.'\t,'A book does not have the ability to show what time it is.',\t\"Books don't tell the time.\"]\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precook(s, n=4, out=False):\n",
    "    \"\"\"Takes a string as input and returns an object that can be given to\n",
    "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
    "    can take string arguments as well.\"\"\"\n",
    "    words = s.split()\n",
    "    counts = defaultdict(int)\n",
    "    for k in range(1,n+1):\n",
    "        for i in range(len(words)-k+1):\n",
    "            ngram = tuple(words[i:i+k])\n",
    "            counts[ngram] += 1\n",
    "    return (len(words), counts)\n",
    "\n",
    "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.'''\n",
    "\n",
    "    reflen = []\n",
    "    maxcounts = {}\n",
    "    for ref in refs:\n",
    "        rl, counts = precook(ref, n)\n",
    "        reflen.append(rl)\n",
    "        for (ngram,count) in six.iteritems(counts):\n",
    "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "    if eff == \"shortest\":\n",
    "        reflen = min(reflen)\n",
    "    elif eff == \"average\":\n",
    "        reflen = float(sum(reflen))/len(reflen)\n",
    "\n",
    "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
    "\n",
    "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
    "\n",
    "    return (reflen, maxcounts)\n",
    "\n",
    "def cook_test(test, reflen_refmaxcounts, eff=None, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.'''\n",
    "\n",
    "    reflen, refmaxcounts = reflen_refmaxcounts\n",
    "    testlen, counts = precook(test, n, True)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "\n",
    "    if eff == \"closest\":\n",
    "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
    "    else: ## i.e., \"average\" or \"shortest\" or None\n",
    "        result[\"reflen\"] = reflen\n",
    "\n",
    "    result[\"testlen\"] = testlen\n",
    "\n",
    "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
    "\n",
    "    result['correct'] = [0]*n\n",
    "    for (ngram, count) in six.iteritems(counts):\n",
    "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
    "\n",
    "    return result\n",
    "\n",
    "class BleuScorer(object):\n",
    "    \"\"\"Bleu scorer.\"\"\"\n",
    "\n",
    "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
    "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
    "\n",
    "    def copy(self):\n",
    "        ''' copy the refs.'''\n",
    "        new = BleuScorer(n=self.n)\n",
    "        new.ctest = copy.copy(self.ctest)\n",
    "        new.crefs = copy.copy(self.crefs)\n",
    "        new._score = None\n",
    "        return new\n",
    "\n",
    "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
    "        ''' singular instance '''\n",
    "\n",
    "        self.n = n\n",
    "        self.crefs = []\n",
    "        self.ctest = []\n",
    "        self.cook_append(test, refs)\n",
    "        self.special_reflen = special_reflen\n",
    "\n",
    "    def cook_append(self, test, refs):\n",
    "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
    "\n",
    "        if refs is not None:\n",
    "            self.crefs.append(cook_refs(refs))\n",
    "            if test is not None:\n",
    "                cooked_test = cook_test(test, self.crefs[-1])\n",
    "                self.ctest.append(cooked_test) ## N.B.: -1\n",
    "            else:\n",
    "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
    "\n",
    "        self._score = None ## need to recompute\n",
    "\n",
    "    def ratio(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._ratio\n",
    "\n",
    "    def score_ratio(self, option=None):\n",
    "        '''return (bleu, len_ratio) pair'''\n",
    "        return (self.fscore(option=option), self.ratio(option=option))\n",
    "\n",
    "    def score_ratio_str(self, option=None):\n",
    "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
    "\n",
    "    def reflen(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._reflen\n",
    "\n",
    "    def testlen(self, option=None):\n",
    "        self.compute_score(option=option)\n",
    "        return self._testlen\n",
    "\n",
    "    def retest(self, new_test):\n",
    "        if type(new_test) is str:\n",
    "            new_test = [new_test]\n",
    "        assert len(new_test) == len(self.crefs), new_test\n",
    "        self.ctest = []\n",
    "        for t, rs in zip(new_test, self.crefs):\n",
    "            self.ctest.append(cook_test(t, rs))\n",
    "        self._score = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rescore(self, new_test):\n",
    "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
    "\n",
    "        return self.retest(new_test).compute_score()\n",
    "\n",
    "    def size(self):\n",
    "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
    "        return len(self.crefs)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''add an instance (e.g., from another sentence).'''\n",
    "\n",
    "        if type(other) is tuple:\n",
    "            ## avoid creating new BleuScorer instances\n",
    "            self.cook_append(other[0], other[1])\n",
    "        else:\n",
    "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
    "            self.ctest.extend(other.ctest)\n",
    "            self.crefs.extend(other.crefs)\n",
    "            self._score = None ## need to recompute\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compatible(self, other):\n",
    "        return isinstance(other, BleuScorer) and self.n == other.n\n",
    "\n",
    "    def single_reflen(self, option=\"average\"):\n",
    "        return self._single_reflen(self.crefs[0][0], option)\n",
    "\n",
    "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
    "\n",
    "        if option == \"shortest\":\n",
    "            reflen = min(reflens)\n",
    "        elif option == \"average\":\n",
    "            reflen = float(sum(reflens))/len(reflens)\n",
    "        elif option == \"closest\":\n",
    "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
    "        else:\n",
    "            assert False, \"unsupported reflen option %s\" % option\n",
    "\n",
    "        return reflen\n",
    "\n",
    "    def recompute_score(self, option=None, verbose=0):\n",
    "        self._score = None\n",
    "        return self.compute_score(option, verbose)\n",
    "\n",
    "    def compute_score(self, option=None, verbose=0):\n",
    "        n = self.n\n",
    "        small = 1e-9\n",
    "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
    "        bleu_list = [[] for _ in range(n)]\n",
    "\n",
    "        if self._score is not None:\n",
    "            return self._score\n",
    "\n",
    "        if option is None:\n",
    "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
    "\n",
    "        self._testlen = 0\n",
    "        self._reflen = 0\n",
    "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
    "\n",
    "        # for each sentence\n",
    "        for comps in self.ctest:\n",
    "            testlen = comps['testlen']\n",
    "            self._testlen += testlen\n",
    "\n",
    "            if self.special_reflen is None: ## need computation\n",
    "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
    "            else:\n",
    "                reflen = self.special_reflen\n",
    "\n",
    "            self._reflen += reflen\n",
    "\n",
    "            for key in ['guess','correct']:\n",
    "                for k in range(n):\n",
    "                    totalcomps[key][k] += comps[key][k]\n",
    "\n",
    "            # append per image bleu score\n",
    "            bleu = 1.\n",
    "            for k in range(n):\n",
    "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
    "                        /(float(comps['guess'][k]) + small)\n",
    "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
    "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
    "            if ratio < 1:\n",
    "                for k in range(n):\n",
    "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(comps, reflen)\n",
    "\n",
    "        totalcomps['reflen'] = self._reflen\n",
    "        totalcomps['testlen'] = self._testlen\n",
    "\n",
    "        bleus = []\n",
    "        bleu = 1.\n",
    "        for k in range(n):\n",
    "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
    "                    / (totalcomps['guess'][k] + small)\n",
    "            bleus.append(bleu ** (1./(k+1)))\n",
    "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
    "        if ratio < 1:\n",
    "            for k in range(n):\n",
    "                bleus[k] *= math.exp(1 - 1/ratio)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(totalcomps)\n",
    "            print(\"ratio:\", ratio)\n",
    "\n",
    "        self._score = bleus\n",
    "        return self._score, bleu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        # default compute Blue score up to 4\n",
    "        self._n = n\n",
    "        self._hypo_for_image = {}\n",
    "        self.ref_for_image = {}\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        bleu_scorer = BleuScorer(n=self._n)\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) >= 1)\n",
    "\n",
    "            bleu_scorer += (hypo[0], ref)\n",
    "\n",
    "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
    "        # print(\"closest:\", score)\n",
    "        # print(\"shortest:\", bleu_scorer.compute_score(option='shortest'))\n",
    "        # print(\"average:\", bleu_scorer.compute_score(option='average', verbose=1))\n",
    "        # return (bleu, bleu_info)\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"Bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: We use book to know the time.\n",
      "Generated Explanation: ['Book is used to know the time.', 'Book is a book.', 'Book is used to know time.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We use book to know the time.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "ground_truths = {test_sentence: ['A book is used to study.'\t,'A book does not have the ability to show what time it is.',\t\"Books don't tell the time.\"]}\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\n",
      "\n",
      "Book is used to know the time.\n",
      "[0.7142857141836736, 0.597614304574709, 0.41491326661265254, 6.500593259109356e-05]\n",
      "\n",
      "Book is a book.\n",
      "[0.19470019567050137, 7.109445940997322e-09, 2.6999515154283422e-11, 1.97867090930997e-12]\n",
      "\n",
      "Book is used to know time.\n",
      "[0.6666666664444446, 0.5163977793135832, 0.4054801328872982, 6.865890476915431e-05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bleu = Bleu()\n",
    "print('[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\\n')\n",
    "for e in explanation:\n",
    "    print(e)\n",
    "    res = {test_sentence: [e]}\n",
    "    print(bleu.compute_score(gts = ground_truths, res = res)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def my_lcs(string, sub):\n",
    "    \"\"\"\n",
    "    Calculates longest common subsequence for a pair of tokenized strings\n",
    "    :param string : list of str : tokens from a string split using whitespace\n",
    "    :param sub : list of str : shorter string, also split using whitespace\n",
    "    :returns: length (list of int): length of the longest common subsequence between the two strings\n",
    "\n",
    "    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n",
    "    \"\"\"\n",
    "    if(len(string)< len(sub)):\n",
    "        sub, string = string, sub\n",
    "\n",
    "    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
    "\n",
    "    for j in range(1,len(sub)+1): \n",
    "        for i in range(1,len(string)+1):\n",
    "            if(string[i-1] == sub[j-1]):\n",
    "                lengths[i][j] = lengths[i-1][j-1] + 1\n",
    "            else:\n",
    "                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
    "\n",
    "    return lengths[len(string)][len(sub)]\n",
    "\n",
    "class Rouge():\n",
    "    '''\n",
    "    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # vrama91: updated the value below based on discussion with Hovey\n",
    "        self.beta = 1.2\n",
    "\n",
    "    def calc_score(self, candidate, refs):\n",
    "        \"\"\"\n",
    "        Compute ROUGE-L score given one candidate and references for an image\n",
    "        :param candidate: str : candidate sentence to be evaluated\n",
    "        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n",
    "        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n",
    "        \"\"\"\n",
    "        assert(len(candidate)==1)\t\n",
    "        assert(len(refs)>0)         \n",
    "        prec = []\n",
    "        rec = []\n",
    "\n",
    "        # split into tokens\n",
    "        token_c = candidate[0].split(\" \")\n",
    "    \t\n",
    "        for reference in refs:\n",
    "            # split into tokens\n",
    "            token_r = reference.split(\" \")\n",
    "            # compute the longest common subsequence\n",
    "            lcs = my_lcs(token_r, token_c)\n",
    "            prec.append(lcs/float(len(token_c)))\n",
    "            rec.append(lcs/float(len(token_r)))\n",
    "\n",
    "        prec_max = max(prec)\n",
    "        rec_max = max(rec)\n",
    "\n",
    "        if(prec_max!=0 and rec_max !=0):\n",
    "            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n",
    "        else:\n",
    "            score = 0.0\n",
    "        return score\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n",
    "        Invoked by evaluate_captions.py \n",
    "        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values \n",
    "        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n",
    "        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n",
    "        \"\"\"\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        score = []\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref  = gts[id]\n",
    "\n",
    "            score.append(self.calc_score(hypo, ref))\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) > 0)\n",
    "\n",
    "        average_score = np.mean(np.array(score))\n",
    "        return average_score, np.array(score)\n",
    "\n",
    "    def method(self):\n",
    "        return \"Rouge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns ROUGE-L Score... how good of match is it with the references?\n",
      "\n",
      "Book is used to know the time.\n",
      "(0.4680306905370844, array([0.46803069]))\n",
      "\n",
      "Book is a book.\n",
      "(0.1930379746835443, array([0.19303797]))\n",
      "\n",
      "Book is used to know time.\n",
      "(0.5, array([0.5]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "print('Returns ROUGE-L Score... how good of match is it with the references?\\n')\n",
    "\n",
    "for e in explanation:\n",
    "    print(e)\n",
    "    res = {test_sentence: [e]}\n",
    "    print(rouge.compute_score(gts = ground_truths, res = res))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: We use book to know the time.\n",
      "Generated Explanation: ['Book is used to know the time.', 'Book is a book.', 'Book is used to know time.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We use book to know the time.\"\n",
    "explanations = generate_explanation(model, tokenizer, test_sentence)\n",
    "ground_truths = {test_sentence: ['A book is used to study.'\t,'A book does not have the ability to show what time it is.',\t\"Books don't tell the time.\"]}\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\n",
      "\n",
      "Book is used to know the time.\n",
      "{'We use book to know the time.': ['Book is a book.', 'Book is used to know time.']}\n",
      "[0.8571428570204083, 0.7559289459014655, 0.6999028046563295, 0.6434588840385813]\n",
      "\n",
      "Book is a book.\n",
      "{'We use book to know the time.': ['Book is used to know the time.', 'Book is used to know time.']}\n",
      "[0.30326532970468434, 0.2476151048074771, 2.6492666763238897e-06, 1.030522425913718e-08]\n",
      "\n",
      "Book is used to know time.\n",
      "{'We use book to know the time.': ['Book is used to know the time.', 'Book is a book.']}\n",
      "[0.8464817246084536, 0.757116271161685, 0.7139503370879647, 0.6731821379696712]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bleu = Bleu()\n",
    "\n",
    "print('[BLEU-1, BLEU-2, BLEU-3, BLEU-4]\\n')\n",
    "for i, e in enumerate(explanations):\n",
    "    print(e)\n",
    "    res = {test_sentence: [e]}\n",
    "    gts = { test_sentence: explanations[:i] + explanations[i+1:]}\n",
    "    print(gts)\n",
    "    print(bleu.compute_score(gts = gts, res = res)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORPUS DIVERSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def eval_entropy_distinct(generated_sentences):\n",
    "    \"\"\"\n",
    "    Computes Entropy-k and Distinct-k for corpus diversity.\n",
    "\n",
    "    :param generated_sentences: List of generated sentences (e.g., model outputs)\n",
    "    :return: Dictionary with entropy and distinct scores for n-grams (1 to 4)\n",
    "    \"\"\"\n",
    "    diversity_metrics = {}\n",
    "    counter = [defaultdict(int) for _ in range(4)]  # Stores n-gram counts\n",
    "\n",
    "    for sentence in generated_sentences:\n",
    "        words = sentence.strip().split()  # Tokenize sentence\n",
    "        for n in range(4):  # n-gram size (1 to 4)\n",
    "            for i in range(len(words) - n):\n",
    "                ngram = ' '.join(words[i:i + n + 1])\n",
    "                counter[n][ngram] += 1\n",
    "\n",
    "    for n in range(4):\n",
    "        total = sum(counter[n].values()) + 1e-10  # Avoid division by zero\n",
    "        \n",
    "        # Entropy-k: Measures evenness of n-gram distribution\n",
    "        entropy_score = -sum((v / total) * np.log(v / total) for v in counter[n].values())\n",
    "        diversity_metrics[f'entropy_{n+1}'] = entropy_score\n",
    "\n",
    "        # Distinct-k: Measures uniqueness of n-grams\n",
    "        diversity_metrics[f'distinct_{n+1}'] = len(counter[n]) / total\n",
    "\n",
    "    return diversity_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Explanations: [\"Stars can't count stars.\", 'Stars do not count stars.', \"Stars can't be counted.\"]\n",
      "Corpus Diversity Scores: {'entropy_1': 1.9915093613489865, 'distinct_1': 0.6153846153798816, 'entropy_2': 2.0253262207598146, 'distinct_2': 0.799999999992, 'entropy_3': 1.9459101490418007, 'distinct_3': 0.9999999999857143, 'entropy_4': 1.3862943611102332, 'distinct_4': 0.999999999975}\n"
     ]
    }
   ],
   "source": [
    "# Generate explanations\n",
    "test_sentence = \"I can count stars.\"\n",
    "explanation = generate_explanation(model, tokenizer, test_sentence)\n",
    "\n",
    "# Compute corpus diversity\n",
    "diversity_scores = eval_entropy_distinct(explanation)\n",
    "\n",
    "# Print results\n",
    "print(\"Generated Explanations:\", explanation)\n",
    "print(\"Corpus Diversity Scores:\", diversity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
